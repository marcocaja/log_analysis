{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29505aa5-251d-45fe-b050-f2f716003650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "[mount for mount in dbutils.fs.mounts() if 'data/' in mount.mountPoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310b380c-2d74-4f0d-a9b8-9e2765d9a033",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c195507c-d39c-418d-9945-9f43d472ebfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fluxo = 'principal'\n",
    "# fluxo = 'escala'\n",
    "\n",
    "if fluxo == 'principal':\n",
    "    source_file = spark.read.text('dbfs:/mnt/data/bronze/access_log.txt')\n",
    "    target_table = 'santander_log_analysis_task'\n",
    "    coalesce = 4\n",
    "elif fluxo == 'escala':\n",
    "    source_file = spark.read.text('dbfs:/mnt/data/data-ingestion/*') \n",
    "    target_table = 'log_analysis'\n",
    "    coalesce = 132\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d83a22-cf55-4b4f-80b6-fdf71164642f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_log_analysis_silver = (\n",
    "    spark.read.load(f'dbfs:/mnt/data/silver/{target_table}')\n",
    "    .coalesce(coalesce)\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n",
    "\n",
    "    df_log_analysis_silver.count() # `count` realizado para forçar o cache de todo o DataFrame\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exceção encontrada durante tentativa de leitura: {e}\")\n",
    "    if 'NOT_FOUND' in str(e):\n",
    "        df_log_analysis_silver = (\n",
    "        source_file\n",
    "        .select(\n",
    "            f.regexp_extract(f.col('value'), r'^([0-9]{1,3}\\.){3}[0-9]{1,3}', 0).alias('ip_address'),\n",
    "            f.regexp_extract(f.col('value'), r'[0-9]{2}\\/[a-zA-Z]{3}\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s-\\d{4}', 0).alias('log_date'),\n",
    "            f.to_timestamp(f.regexp_extract(f.col('value'), r'[0-9]{2}\\/[a-zA-Z]{3}\\/\\d{4}:\\d{2}:\\d{2}:\\d{2}\\s-\\d{4}', 0), 'dd/MMM/yyyy:HH:mm:ss Z').alias('log_date_offseted'),\n",
    "            f.regexp_extract(f.col('value'), r'\\\"(\\w*)\\s', 1).alias('http_method'),\n",
    "            f.regexp_extract(f.col('value'), r'(?:\\w*) (\\/.*)\\sHTTP', 1).alias('http_endpoint'),\n",
    "            f.regexp_extract(f.col('value'), r'\\s(\\d{3})\\s([-\\d]*)', 1).alias('http_response_code'),   \n",
    "            f.when(f.regexp_extract(f.col('value'), r'\\s(\\d{3})\\s([-\\d]*)', 2) == '-', f.lit(0)).otherwise(f.regexp_extract(f.col('value'), r'\\s(\\d{3})\\s([-\\d]*)', 2)).alias('http_response_size'),\n",
    "            f.regexp_extract(f.col('value'), r'HTTP\\/\\d.\\d', 0).alias('http_version'),\n",
    "        ).select(\"*\",\n",
    "                f.when(f.regexp(f.col('http_endpoint'), f.lit(r\"\\.\\w{1,}\")), f.lit(1)).otherwise(f.lit(0)).alias('is_file'),\n",
    "                f.when(f.col('http_response_size') > 0, f.lit(1)).otherwise(f.lit(0)).alias('has_response_size'),             \n",
    "                f.when(f.col('http_response_code').rlike(r'^1'), f.lit('info'))\n",
    "                .when(f.col('http_response_code').rlike(r'^2'), f.lit('success'))\n",
    "                .when(f.col('http_response_code').rlike(r'^3'), f.lit('redirect'))\n",
    "                .when(f.col('http_response_code').rlike(r'^4'), f.lit('client-error'))\n",
    "                .otherwise(f.lit('server-error')).alias('http_response_type')\n",
    "            )\n",
    "        .withColumn('http_endpoint', \n",
    "                    f.when(f.length(f.col('http_endpoint')) > 1, f.regexp_replace(f.col('http_endpoint'), r'\\/$', ''))\n",
    "                    .otherwise(f.col('http_endpoint'))) # Remove a barra ao final, para evitar confusão de endpoints, como: /search/ e /search, que possuem o mesmo efeito prático    \n",
    "    )\n",
    "        (\n",
    "            df_log_analysis_silver\n",
    "            .write\n",
    "            .format('delta')\n",
    "            .mode('overwrite')\n",
    "            .option('overwriteSchema', 'true')\n",
    "            .option('delta.targetFileSize', '128MB')\n",
    "            .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "            .option('delta.dataSkippingStatsColumns', 'ip_address,log_date,log_date_offseted,http_method,http_response_size,is_file,has_response_size,http_response_type')\n",
    "            .save(f'dbfs:/mnt/data/silver/{target_table}') \n",
    "        )\n",
    "\n",
    "        spark.sql(f\"OPTIMIZE DELTA.`dbfs:/mnt/data/silver/{target_table}` ZORDER BY (log_date_offseted, ip_address, is_file, http_response_type, has_response_size)\")\n",
    "    else:\n",
    "        raise Exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2928d92e-6c8d-4769-b907-cb9c13e18a94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. **Identifique as 10 maiores origens de acesso (Client IP) por quantidade de acessos.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52d95ca-4476-4844-9a7a-d5abe06b12a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC53aW5kb3cgaW1wb3J0IFdpbmRvdwp0cnk6CiAgICBkZl90b3BfdGVuX2lwc19tb3N0X2FjY2VzcyA9IHNwYXJrLnJlYWQubG9hZChmJ2RiZnM6L21udC9kYXRhL2dvbGQve3RhcmdldF90YWJsZX1fdG9wX3Rlbl9pcHNfbW9zdF9hY2Nlc3MnKQoKZXhjZXB0OgoKICAgIGRmX3RvcF90ZW5faXBzX21vc3RfYWNjZXNzID0gKAogICAgICAgIGRmX2xvZ19hbmFseXNpc19zaWx2ZXIKICAgICAgICAuZ3JvdXBCeSgnaXBfYWRkcmVzcycpCiAgICAgICAgLmFnZyhmLmNvdW50KCdpcF9hZGRyZXNzJykuYWxpYXMoJ2FjY2Vzc19mcmVxdWVuY3knKSkKICAgICAgICAud2l0aENvbHVtbigncmFuaycsIGYuZGVuc2VfcmFuaygpLm92ZXIoV2luZG93Lm9yZGVyQnkoZi5jb2woJ2FjY2Vzc19mcmVxdWVuY3knKS5kZXNjKCkpKSkgIyBBZGnDp8OjbyBkZSByYW5raW5nIHBhcmEgc2VyIG1haXMgZsOhY2lsIGRlIG1hbmlwdWxhciBhIGNsYXNzaWZpY2HDp8OjbwogICAgICAgIC5vcmRlckJ5KGYuY29sKCdhY2Nlc3NfZnJlcXVlbmN5JykuZGVzYygpKQogICAgICAgIC5maWx0ZXIoInJhbmsgPD0gMTAiKQogICAgKQoKICAgICMgKAogICAgIyAgICAgZGZfdG9wX3Rlbl9pcHNfbW9zdF9hY2Nlc3MKICAgICMgICAgIC53cml0ZQogICAgIyAgICAgLmZvcm1hdCgnZGVsdGEnKQogICAgIyAgICAgLm1vZGUoJ292ZXJ3cml0ZScpCiAgICAjICAgICAub3B0aW9uKCdvdmVyd3JpdGVTY2hlbWEnLCAndHJ1ZScpCiAgICAjICAgICAub3B0aW9uKCdkZWx0YS50YXJnZXRGaWxlU2l6ZScsICcxMjhNQicpCiAgICAjICAgICAub3B0aW9uKCdkZWx0YS5hdXRvT3B0aW1pemUub3B0aW1pemVXcml0ZScsICd0cnVlJykKICAgICMgICAgIC5zYXZlKGYnZGJmczovbW50L2RhdGEvZ29sZC97dGFyZ2V0X3RhYmxlfV90b3BfdGVuX2lwc19tb3N0X2FjY2VzcycpIAogICAgIyApCgpkaXNwbGF5KGRmX3RvcF90ZW5faXBzX21vc3RfYWNjZXNzKQo=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewdf55432\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewdf55432\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewdf55432\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewdf55432) SELECT `ip_address`,SUM(`access_frequency`) `column_2f505a4618` FROM q GROUP BY `ip_address`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewdf55432\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "IPs com mais acessos",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "ip_address",
             "id": "column_2f505a4616"
            },
            "y": [
             {
              "column": "access_frequency",
              "id": "column_2f505a4618",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_2f505a4618": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": false,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "access_frequency"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "0254193b-5992-4b88-8526-294ca9e98683",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "ip_address",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "ip_address",
           "type": "column"
          },
          {
           "alias": "column_2f505a4618",
           "args": [
            {
             "column": "access_frequency",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": null,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "try:\n",
    "    df_top_ten_ips_most_access = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_top_ten_ips_most_access')\n",
    "\n",
    "except:\n",
    "\n",
    "    df_top_ten_ips_most_access = (\n",
    "        df_log_analysis_silver\n",
    "        .groupBy('ip_address')\n",
    "        .agg(f.count('ip_address').alias('access_frequency'))\n",
    "        .withColumn('rank', f.dense_rank().over(Window.orderBy(f.col('access_frequency').desc()))) # Adição de ranking para ser mais fácil de manipular a classificação\n",
    "        .orderBy(f.col('access_frequency').desc())\n",
    "        .filter(\"rank <= 10\")\n",
    "    )\n",
    "\n",
    "    # (\n",
    "    #     df_top_ten_ips_most_access\n",
    "    #     .write\n",
    "    #     .format('delta')\n",
    "    #     .mode('overwrite')\n",
    "    #     .option('overwriteSchema', 'true')\n",
    "    #     .option('delta.targetFileSize', '128MB')\n",
    "    #     .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "    #     .save(f'dbfs:/mnt/data/gold/{target_table}_top_ten_ips_most_access') \n",
    "    # )\n",
    "\n",
    "display(df_top_ten_ips_most_access)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da147bdb-5595-4954-b1e4-0caec4c782f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. **Liste os 6 endpoints mais acessados, desconsiderando aqueles que representam arquivos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e93cf82-b81d-46b2-bec8-e1549c5eea9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_top_six_endpoints = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_top_six_endpoints')\n",
    "except:\n",
    "\n",
    "    df_top_six_endpoints = (\n",
    "        df_log_analysis_silver\n",
    "        .where(\"is_file = 0\")\n",
    "        # .where(\"is_file = 0 AND http_response_type = 'success'\") # acho que faz mais sentido filtrar pelas páginas que tiveram sucesso, excluindo retornos info, redirect e error. Até porque 'is_file' é um parâmetro de z-ordering\n",
    "        .groupBy('http_endpoint')\n",
    "        .agg(f.count('http_endpoint').alias('http_endpoint_frequency'))\n",
    "        .withColumn('rank', f.dense_rank().over(Window.orderBy(f.col('http_endpoint_frequency').desc())))\n",
    "        .orderBy(f.col('http_endpoint_frequency').desc())\n",
    "        .filter(\"rank <= 6\")\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_top_six_endpoints\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_top_six_endpoints') \n",
    "    )\n",
    "\n",
    "print('Endpoints mais acessados')\n",
    "display(df_top_six_endpoints)\n",
    "print('Endpoints mais acessados excluindo a raíz')\n",
    "display(df_top_six_endpoints.filter(\"http_endpoint != '/'\")) # Também é possível elencar os endpoints que não são a página inicial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b3dad6d-1a34-44d1-97d1-7a8eb603622d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Endpoints com mais erros, divididos por client-side e server-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e434f3e-3716-48e4-9f0a-b1fe61af4ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_top_six_failing_endpoints = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_top_six_failing_endpoints') \n",
    "\n",
    "except:\n",
    "\n",
    "    df_top_six_failing_endpoints = (\n",
    "        df_log_analysis_silver\n",
    "        .where(\"is_file = 0 AND http_response_type in ('client-error', 'server-error')\")\n",
    "        .groupBy('http_endpoint', 'http_response_type')\n",
    "        .agg(f.count('http_endpoint').alias('http_endpoint_frequency'))\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_top_six_failing_endpoints\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_top_six_failing_endpoints') \n",
    "    )\n",
    "\n",
    "print('Client-errors')\n",
    "display(\n",
    "    df_top_six_failing_endpoints\n",
    "    .filter(\"http_response_type = 'client-error'\")\n",
    "    .withColumn('rank', f.dense_rank().over(Window.orderBy(f.col('http_endpoint_frequency').desc())))\n",
    "    .orderBy(f.col('http_endpoint_frequency').desc())\n",
    "    .filter(\"rank <= 6\")    \n",
    ")\n",
    "\n",
    "print('Server-errors')\n",
    "display(\n",
    "    df_top_six_failing_endpoints\n",
    "    .filter(\"http_response_type = 'server-error'\")\n",
    "    .withColumn('rank', f.dense_rank().over(Window.orderBy(f.col('http_endpoint_frequency').desc())))\n",
    "    .orderBy(f.col('http_endpoint_frequency').desc())\n",
    "    .filter(\"rank <= 6\")    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc85648-1170-4dca-b5f7-fda9f276d201",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2.2 Arquivos mais consumidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4824659-4563-49ce-95b9-5d5bc7d5025d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_top_10_most_accessed_files = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_top_10_most_accessed_files')\n",
    "except:\n",
    "    df_top_10_most_accessed_files = (\n",
    "        df_log_analysis_silver\n",
    "        .where(\"is_file = 1\")\n",
    "        .groupBy('http_endpoint', 'http_response_type')\n",
    "        .agg(f.count('http_endpoint').alias('http_endpoint_frequency'))\n",
    "        .withColumn('rank', f.dense_rank().over(Window.orderBy(f.col('http_endpoint_frequency').desc())))\n",
    "        .orderBy(f.col('http_endpoint_frequency').desc())\n",
    "        .filter(\"rank <= 10\")    \n",
    "    )\n",
    "\n",
    "    # Escrita da tabela na camada gold\n",
    "    (\n",
    "        df_top_10_most_accessed_files\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_top_10_most_accessed_files') \n",
    "    )\n",
    "\n",
    "display(df_top_10_most_accessed_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a597654-764a-48b8-bd78-2fbdc31177df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. **Qual a quantidade de Client IPs distintos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fbc0eee-a1a4-43aa-bb11-a03f45ee77bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_count_distinct_ips = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_count_distinct_ips')\n",
    "\n",
    "except:\n",
    "    df_count_distinct_ips = (\n",
    "        df_log_analysis_silver\n",
    "        .groupBy()\n",
    "        .agg(f.countDistinct('ip_address').alias('distinct_ips'))\n",
    "    )\n",
    "\n",
    "    # Escrita da tabela na camada gold\n",
    "    (\n",
    "        df_count_distinct_ips\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_count_distinct_ips') \n",
    "    )\n",
    "\n",
    "display(df_count_distinct_ips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c5d2d3-5c21-42bd-a1e1-5e4afdc3fd80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. **Quantos dias de dados estão representados no arquivo?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9f4193-83a8-4e97-b8fd-fa0f498571f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_days_between_log_dates = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_days_between_log_dates')\n",
    "except:\n",
    "    df_days_between_log_dates = (\n",
    "        df_log_analysis_silver\n",
    "        .groupBy()\n",
    "        .agg(f.min('log_date_offseted').alias('earliest_date'),\n",
    "            f.max('log_date_offseted').alias('latest_date'),\n",
    "            f.date_diff(f.max('log_date_offseted'), f.min('log_date_offseted')).alias('days_between_dates'))\n",
    "    )\n",
    "\n",
    "    # Escrita da tabela na camada gold\n",
    "    (\n",
    "        df_days_between_log_dates\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_days_between_log_dates') \n",
    "    )\n",
    "\n",
    "display(df_days_between_log_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2f206f-c2df-481c-99c9-f18b38fc00e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5. **Com base no tamanho (em bytes) do conteúdo das respostas, faça a seguinte análise:**\n",
    "   - O volume total de dados retornado.\n",
    "   - O maior volume de dados em uma única resposta.\n",
    "   - O menor volume de dados em uma única resposta.\n",
    "   - O volume médio de dados retornado.\n",
    "   - *Dica:* Considere como os dados podem ser categorizados por tipo de resposta para realizar essas análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7526b4d1-087f-4407-a763-24e53e030f1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df_log_analysis_response_sizes = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_log_analysis_response_sizes')\n",
    "\n",
    "except: \n",
    "    df_log_analysis_response_sizes = (\n",
    "        df_log_analysis_silver\n",
    "        .where(\"has_response_size = 1\")\n",
    "        .groupBy()\n",
    "        .agg(\n",
    "            f.sum(f.col('http_response_size')).alias('total_response_size'),\n",
    "            f.max(f.col('http_response_size')).alias('max_response_size'),\n",
    "            f.min(f.col('http_response_size')).alias('min_response_size'),\n",
    "            f.avg(f.col('http_response_size')).alias('avg_response_size'),\n",
    "        )\n",
    "    )\n",
    "\n",
    "        # Escrita da tabela na camada gold\n",
    "    (\n",
    "        df_log_analysis_response_sizes\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_log_analysis_response_sizes') \n",
    "    )\n",
    "\n",
    "display(df_log_analysis_response_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dda61bf-0a16-45df-90a8-8073b1112d36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6. **Qual o dia da semana com o maior número de erros do tipo \"HTTP Client Error\"?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5f0840-1fbb-476c-bc49-a95f0962403a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView430d97c\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView430d97c\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView430d97c\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView430d97c) SELECT `weekday`,SUM(`weekday_client_error_count`) `column_2f505a466` FROM q GROUP BY `weekday`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView430d97c\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualização 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "weekday",
             "id": "column_2f505a465"
            },
            "y": [
             {
              "column": "weekday_client_error_count",
              "id": "column_2f505a466",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_2f505a466": {
             "name": "weekday_client_error_count",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": false,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "f8844926-e1c1-4b21-b012-d3e343f1e5b2",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "weekday",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "weekday",
           "type": "column"
          },
          {
           "alias": "column_2f505a466",
           "args": [
            {
             "column": "weekday_client_error_count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": null,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "try: \n",
    "    df_week_day_with_more_client_errors = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_week_day_with_more_client_errors')\n",
    "\n",
    "except:\n",
    "    from pyspark.sql.types import StringType\n",
    "\n",
    "    @f.udf(StringType())\n",
    "    def get_weekday(day_of_week: int) -> str:\n",
    "        \"\"\"\n",
    "        Retorna o dia da semana como a string com o nome, a partir de uma entrada de um número inteiro entre 0 e 6.\n",
    "\n",
    "        Parameters:\n",
    "            day_of_week (int): O número do dia da semana, entre 0 e 6.\n",
    "        \n",
    "        Returns:\n",
    "            str: O nome do dia da semana, como uma string.\n",
    "        \"\"\"\n",
    "        assert day_of_week >= 0 and day_of_week <= 6, \"day_of_week must be between 0 and 6\"\n",
    "        days = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "        return days[day_of_week]\n",
    "\n",
    "\n",
    "    df_week_day_with_more_client_errors = (\n",
    "        df_log_analysis_silver\n",
    "        .where(\"http_response_type = 'client-error'\")\n",
    "        .groupBy(get_weekday(f.weekday(f.col('log_date_offseted'))).alias('weekday'))\n",
    "        .agg(f.count(get_weekday(f.weekday(f.col('log_date_offseted')))).alias('weekday_client_error_count'))\n",
    "        .orderBy('weekday_client_error_count', ascending=False)\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_week_day_with_more_client_errors\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_week_day_with_more_client_errors') \n",
    "    )\n",
    "\n",
    "\n",
    "display(week_day_with_more_client_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f12803-91a7-4e72-954a-265593b5cc54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6.1 O quanto conseguimos explorar a diarização?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f2c09a7-c1f2-4e9a-b453-85364dc0560a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2b4eb88\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2b4eb88\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2b4eb88\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2b4eb88) SELECT `date_offseted`,SUM(`http_response_type_count`) `column_2f505a4629`,`http_response_type` FROM q GROUP BY `date_offseted`,`http_response_type`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2b4eb88\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Requisições por tipo e dia",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "http_response_type",
             "id": "column_2f505a4631"
            },
            "x": {
             "column": "date_offseted",
             "id": "column_2f505a4627"
            },
            "y": [
             {
              "column": "http_response_type_count",
              "id": "column_2f505a4629",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "client-error": {
             "color": "#AB4057",
             "yAxis": 0
            },
            "column_2f505a4629": {
             "type": "line",
             "yAxis": 0
            },
            "redirect": {
             "color": "#077A9D",
             "yAxis": 0
            },
            "server-error": {
             "color": "#FF3621",
             "yAxis": 0
            },
            "success": {
             "color": "#00A972",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Data"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "#Quantidade de requisições"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "4d1d558a-3aab-4ea9-95da-32537e846bc8",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date_offseted",
           "type": "column"
          },
          {
           "column": "http_response_type",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date_offseted",
           "type": "column"
          },
          {
           "alias": "column_2f505a4629",
           "args": [
            {
             "column": "http_response_type_count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "http_response_type",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": null,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Originalmente, o código foi estruturado em PySpark, conforme definição abaixo, mas preferi a alteração para uma query SQL para facilitação da subquery (evitando o uso do collect)\n",
    "\n",
    "# Definição anterior, também funcional:\n",
    "# max_process_date, max_process_date_minus_window  = (\n",
    "#     df_log_analysis_silver\n",
    "#     .select(f.to_date(f.max('log_date_offseted')).alias('max_process_date'), f.date_sub(f.to_date(f.max('log_date_offseted')), moving_window).alias('max_process_date_minus_window'))\n",
    "#     .collect()[0]\n",
    "# )\n",
    "\n",
    "\n",
    "# df_log_analysis_type_and_date = (\n",
    "    # df_log_analysis_silver\n",
    "    # .groupBy(\n",
    "    #     f.to_date(f.col('log_date_offseted')).alias('date_offseted'),\n",
    "    #     f.col('http_response_type'),\n",
    "    #     get_weekday(f.weekday(f.col('log_date_offseted'))).alias('weekday')\n",
    "    # )\n",
    "    # .agg(f.count(f.col('http_response_type').alias('http_response_type')))\n",
    "    # .filter(f.col('date_offseted').between(max_process_date_minus_window, max_process_date)\n",
    "    # )\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    df_log_analysis_type_and_date = spark.read.load(f'dbfs:/mnt/data/gold/{target_table}_log_analysis_type_and_date')\n",
    "\n",
    "except:\n",
    "    moving_window = 30 # é possível modificar a janela para aumentar ou diminuir o horizonte de análise\n",
    "    df_log_analysis_silver.createOrReplaceTempView('log_analysis_type_and_date')\n",
    "\n",
    "    df_log_analysis_type_and_date = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT \n",
    "            TO_DATE(log_date_offseted) AS date_offseted,\n",
    "            http_response_type,\n",
    "            CASE \n",
    "                WHEN WEEKDAY(log_date_offseted) = 0 THEN 'Monday'\n",
    "                WHEN WEEKDAY(log_date_offseted) = 1 THEN 'Tuesday'\n",
    "                WHEN WEEKDAY(log_date_offseted) = 2 THEN 'Wednesday'\n",
    "                WHEN WEEKDAY(log_date_offseted) = 3 THEN 'Thursday'\n",
    "                WHEN WEEKDAY(log_date_offseted) = 4 THEN 'Friday'\n",
    "                WHEN WEEKDAY(log_date_offseted) = 5 THEN 'Saturday'\n",
    "                WHEN WEEKDAY(log_date_offseted) = 6 THEN 'Sunday'\n",
    "            END AS weekday,\n",
    "            COUNT(http_response_type) AS http_response_type_count\n",
    "        FROM \n",
    "            log_analysis_type_and_date\n",
    "        WHERE \n",
    "            TO_DATE(log_date_offseted) BETWEEN (\n",
    "                    SELECT date_sub(MAX(TO_DATE(log_date_offseted)), {moving_window})  FROM log_analysis_type_and_date\n",
    "                ) AND (\n",
    "                    SELECT MAX(TO_DATE(log_date_offseted)) FROM log_analysis_type_and_date\n",
    "                )\n",
    "        GROUP BY \n",
    "            TO_DATE(log_date_offseted),\n",
    "            http_response_type\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    (\n",
    "        df_log_analysis_type_and_date\n",
    "        .write\n",
    "        .format('delta')\n",
    "        .mode('overwrite')\n",
    "        .option('overwriteSchema', 'true')\n",
    "        .option('delta.targetFileSize', '128MB')\n",
    "        .option('delta.autoOptimize.optimizeWrite', 'true')\n",
    "        .save(f'dbfs:/mnt/data/gold/{target_table}_log_analysis_type_and_date') \n",
    "    )\n",
    "\n",
    "display(df_log_analysis_type_and_date)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "log_analysis",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
